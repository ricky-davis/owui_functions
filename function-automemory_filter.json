[{"id":"automemory_filter","user_id":"70fc7f11-b212-4c80-bedc-ae872d103f4f","name":"AutoMemory","type":"filter","content":"\"\"\"\ntitle: AutoMemory\nauthor: Ricky Davis / spyci\nauthor_url: https://github.com/ricky-davis/owui_functions\nfunding_url: https://github.com/ricky-davis/owui_functions\nversion: 1.2\n\"\"\"\n\n\"\"\"\nThis filter works by sending your message to the LLM with a special prompt to find and extract memories.\nAs such, use of this filter will increase inferencing costs.\n\"\"\"\n\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nfrom apps.webui.models.users import Users\nfrom apps.webui.routers.memories import add_memory, AddMemoryForm\nfrom fastapi.requests import Request\nfrom main import webui_app\nfrom main import generate_chat_completions\nimport json\n\n\nclass Filter:\n    def __init__(self):\n        pass\n\n    async def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict:\n        # Modify the request body or validate it before processing by the chat completion API.\n        # This function is the pre-processor for the API where various checks on the input can be performed.\n        # It can also modify the request before sending it to the API.\n        # print(f\"inlet:{__name__}\")\n        # print(f\"inlet:body:{body}\")\n        # print(f\"inlet:user:{user}\")\n\n        user = Users.get_user_by_id(__user__[\"id\"])\n        content = body[\"messages\"][-1][\"content\"]\n\n        try:\n            formdata = {\n                \"model\": body[\"model\"],\n                \"stream\": False,\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"\"\"\nParse the following content and extract personal information or significant details like names, ages, locations, preferences, desires, fears, medical history, appointments, or anything the user explicitly asks you to remember.\nBe strict and only grab the most important memories.\nKeep all memories short and simple, do not combine them, do not have long runon sentences.\nMemories must *always* start with the word \"User\" or \"User's\".\nIt is preferrable to return nothing over returning garbage, useless memories. If anything is not specified, don't bother including it.\nIf user says \"Your\" or \"You\", they mean the Assistant.\n\nHere are some examples of user messages and the memories they should create. Use these as a baseline for creating all memories:\n\"My name is Jeff\" = \"User's name is Jeff\"\n\"Your name is now Bob\" = \"User has named Assistant 'Bob'\"\n\"My favorite color is red\" = \"User's favorite color is red\"\n\"I'm 28 years old\" = \"User's age is 28\"\n\"I'm a sucker for a good sci-fi novel or an immersive RPG\" = [\"User loves sci-fi novels\", \"User loves immersive RPG's\"]\n\n\nReturn a structured representation of the data in the schema below. Do not surround the JSON in codeblocks or ```\nContent:\n---\n{content}\n---\nAnswer in JSON using this schema:\n{{\n  // In format 'Users <property/detail> is <value>' like 'Users age is 30'\n  memories: string[],\n}}\nJSON:\n\"\"\",\n                    }\n                ],\n            }\n\n            result = await generate_chat_completions(formdata, user)\n            if isinstance(result, dict):\n                # print(f\"{result=}\")\n                content = result[\"choices\"][0][\"message\"][\"content\"]\n            else:\n                lines = []\n                async for line in result.body_iterator:\n                    print(f\"{line=}\")\n                    lines.append(json.loads(line.decode(\"utf-8\").strip()))\n                # print(f\"{lines=}\")\n                content = lines[0][\"choices\"][0][\"message\"][\"content\"]\n            # print(f\"{content=}\")\n            content = json.loads(content)\n            # print(f\"{content=}\")\n\n            memories = content[\"memories\"]\n            print(f\"{memories=}\")\n            if len(memories):\n                for memory in memories:\n                    try:\n                        memory_obj = await add_memory(\n                            request=Request(scope={\"type\": \"http\", \"app\": webui_app}),\n                            form_data=AddMemoryForm(content=memory),\n                            user=user,\n                        )\n                        print(f\"Memory Added: {memory_obj}\")\n                    except Exception as e:\n                        print(f\"Error adding memory {str(e)}\")\n            else:\n                print(\"\")\n\n        except Exception as e:\n            print(f\"Error formatting memory {str(e)}\")\n            return body\n\n        return body\n","meta":{"description":"Pre-prompts the LLM to automatically find memories to remember. Use of this filter will increase inferencing costs.","manifest":{"title":"AutoMemory","author":"Ricky Davis / spyci","author_url":"https://github.com/ricky-davis/owui_functions","funding_url":"https://github.com/ricky-davis/owui_functions","version":"1.2"}},"is_active":true,"is_global":false,"updated_at":1719523611,"created_at":1719523356}]